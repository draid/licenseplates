\documentclass{beamer}

\mode<presentation> {
  \usetheme{Singapore}
  %\setbeamercovered{transparent}
  %\usecolortheme{wolverine}
}

\usepackage{palatino}
\usepackage{graphicx}

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}

\title{Identifying License Plates Ids with Hidden Markov
    Models}
\author{ James Pritts and Ondrej Hrstka }
\institute[CTU FEE]{Czech Technical University in Prague - Faculty of Electrical Engineering}
\date{13.2.2012}

\begin{document}
\begin{frame}
  \titlepage
\end{frame}


\section{Introduction}

\begin{frame}
  \frametitle{Problem statement}
\item[Task] Report the unique identifying string of characters, called
  the \emph{vehicle-id}, from the image of a license plate.

\item[Input] Images of license plates that have been segmented and
  ortho-rectified.
\end{frame}

\begin{frame}
  \frametitle{Assumptions about data}
\item The font of all characters across license plates is identical.
\item Examplars of a particular character are the same width (we relax this restriction later).
\item All characters have the same height.
\item There is significant white space between adjacent characters.
\end{frame}

\begin{frame}
  \frametitle{Hidden Markov Model}

\item The left-to-right sequence of consecutive intensity columns from
  the segmented image of the license plate are the
  \emph{observations}.

\item The set of \emph{hidden states} $\mathbf{K}$ are the column
  indexes of all the glyphs in the font set and an added state $w$
  that represents white-space.

\[S =
\left(\,\dots,\text{w},\text{A}_1,\text{A}_2,\ldots,\text{A}_{m},\text{w},\text{w},\text{w},\text{T}_1,\text{T}_2,\ldots\,\right).\]


\item The \emph{emission probabilities} are the probabilities of
  observing pixel intensities given their corresponding glyph column,
  or, equivalently, given their \emph{hidden states}

\end{frame}

\section{Learning the model}

\begin{frame}
  \frametitle{Input dataset}
  Input dataset is contains 5000 annotated examples. Each example is structure containing:
\begin{itemize}
  \item Original license plate images
  \item Cropped and normalized image
  \item Characters string with bounding boxes for each character.
\end{itemize}

\begin{figure}
\includegraphics[width=\linewidth]{pics/input_example.eps}
\caption{Example of annotated input data. Normalized image displayed.}
\label{fig:distribution}
\end{figure}

\end{frame}

\begin{frame}
  \frametitle{Difficult data}
\begin{figure}
\includegraphics[width=0.8\linewidth]{pics/bad1.eps}
\end{figure}

\begin{figure}
\includegraphics[width=0.8\linewidth]{pics/bad2.eps}
\end{figure}

\begin{figure}
\includegraphics[width=0.8\linewidth]{pics/bad3.eps}
\end{figure}

\end{frame}


\begin{frame}
  \frametitle{Learning the emission probabilities}
	Emission probabilities of the model were learened using EM algorithm.
  
\end{frame}

\begin{frame}
  \frametitle{Learning the emission probabilities}
\begin{block}{E-step}
	\[
  p^{t+1}(x^d_{ij} \mid c=f, s_j) =
  \frac{\mathcal{N}(x^d_{ij};\sigma_1^t,\mu_1^t)\gamma_{is_j}^t}{\mathcal{N}(x^d_{ij};\sigma_1^t,\mu_1^t)\gamma_{is_j}^t+\mathcal{N}(x^d_{ij};\sigma_2^t,\mu_2^t)(1-\gamma_{is_j}^t)}
\]
	\end{block}

\begin{block}{M-step}
\[
  \gamma_{is_j}^{t+1} = \frac{\sum_{d \in \mathcal{D}} p^{t+1}(x^d_{ij} \mid
    c=f, s_j)}{\sum_{d \in \mathcal{D}} p^{t+1}(x^d_{ij} \mid c=f,
    s_j) + \sum_{d \in \mathcal{D}} (1-p^{t+1}(x^d_{ij} \mid c=b,
    s_j))}
\]
\[
  \sigma_1^{t+1} = \frac{\sum_{i,j} \sum_{d \in \mathcal{D}} x^d_{
      ij} p^{t+1}(x^d_{ij} \mid c=f, s_j)}{\sum_{i,j} \sum_{d \in
      \mathcal{D}} p^{t+1}(x^d_{ij} \mid c=f, s_j)}
\]
\end{block}  
\end{frame}

\begin{frame}
  \frametitle{Learning the emission probabilities}
\begin{itemize}
\item Stopping criterion is relative
\item  $|1-\frac{\sigma_1^{t+1}}{\sigma_1^{t}}| < \epsilon \wedge |1-\frac{\sigma_2^{t+1}}{\sigma_2^{t}}| < \epsilon$
\item$\epsilon$ has been set to value $10^{-5}$
\end{itemize}  
\end{frame}

\begin{frame}
  \frametitle{Learned templates}
\begin{figure}[htp]
\centering
\includegraphics[width=\linewidth]{pics/jba.png}
\caption{Learned templates of \texttt{J}, \texttt{B} and
  \texttt{A}. Intensity corresponds to the prior probability for being
  a foreground pixel.}
\label{fig:templates}
\end{figure}
\end{frame}




\begin{frame}
  \frametitle{Setting the transition probabilities}
  JAMES
\end{frame}


\begin{frame}
  \frametitle{Inference}
  JAMES
\end{frame}

\section{Evaluation}
\begin{frame}
  \frametitle{Evaluation}
  
\end{frame}

\begin{frame}
  \frametitle{Conclusions}
\end{frame}

\end{document}
