\documentclass[a4paper,12pt]{article}

\usepackage[left=1in, right=1in, top=0.5in, bottom=1in]{geometry}
\usepackage[latin2]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{color}
\usepackage{amssymb}
%Some useful macros

\definecolor{Cgreen}{rgb}{0,0.6,0}
\definecolor{Cblue}{rgb}{0,0.39,0.61}

\newcounter{ohNoteCounter}
\newcommand{\ohnote}[1]{{\scriptsize  \color{Cgreen} $\clubsuit$~\refstepcounter{ohNoteCounter}\textsf{[OH]$_{\arabic{ohNoteCounter}}$:{#1}}}}

\newcounter{jpNoteCounter}
\newcommand{\jpnote}[1]{{\scriptsize  \color{Cblue} $\blacksquare$ \refstepcounter{jpNoteCounter}\textsf{[JP]$_{\arabic{jpNoteCounter}}$:{#1}}}}

\IfFileExists{.notes_disabled}{
	\renewcommand{\jpnote}[1]{}
	\renewcommand{\ohnote}[1]{}
}



\usepackage{times}

\begin{document}
  \pagestyle{empty}
  \ohnote{This is useful way to leave notes. Both of us have command tu leave notes. I have \textbackslash ohnote, you have \textbackslash jpnote. Notes in whole document can be disabled by creating file with name ``.notes\_disabled''. }

  \jpnote{Your notes will look like this }

  \section{Preliminaries}
  Let $f : \mathbf{x} \mapsto \mathcal{R}$ be a function that maps a point $\mathbf{u} = (u_j,u_i)$ to a real value $x_{ij}$, where $(u_j,u_i) \in [0,N_j] \otimes [0,N_i]$.  So $f$ gives the raw intensity value at each point in the image.  Denote $\mathbf{x_i}$ and$\mathbf{x_j}$  as the row and column of pixels at row $i$ and column $j$ respectively.  

  Define the Markov model as 

  \begin{eqnarray*}
    \mathcal{K} &=& \{ s_k | \text{\,column\,} k \text{\,of a letter, or column of whitespace \,} \} \\
    P(x_{ij}) &=& \text{probability of observing pixel intensity\,} x_{ij} \\
    P(s_k) &=& \text{probability of observing hidden state \,} s_{k} \\
    P(x_{ij}|s_k) &=& \text{probability of observing pixel intensity\,} x_{ij} \text{\,given\,} s_k  
  \end{eqnarray*}

  \section{Preprocessing}
  \subsection{Photometric Normalization}

  \section{Training}
  \subsection{Overview}
  Jimmy

  \subsection{Expectation Maximization for Learning Emission probability distributions}
  \ohnote{Here are some notes, that we might use. It's not complete text} 

  \[
   p(x_{ij}) = p(x_{ij} | s_k) p(s_k) + p(x_{ij}|\overline{s_k}) p(\overline{s_k}),
  \]
  where $\overline{s_k}$ represents complement to $s_k$ (pixel is not in $s_k$).

  \ohnote{I don't know about this, what about another approach. I am stuck here actually. :-(
  I describe this on example, if we agree on this I rewrite it in general form. }

  We have $n$ observations (training set) of the first column of letter A. Let this be multi-set $\mathcal{O}_{A,1} = \{\mathbf{x_1},...,\mathbf{x_n}\}$, where $\mathbf{x_{i,j}}$ is pixel in $j$-th row. For template $s_{A,1}$ stands
  \[
    p(\mathbf{x_{A,1}}|s_{A,1}) = \prod_{i=1}^{k} p(x_{A,1,i}|s_{A,1}).
  \]
  We need to train $p(x_{A,1,i}|s_{A,1})$ somehow. The most naive way is probably mean value...
  


  \subsection{Setting Transition probabilities}
\end{document}

%--------------------------------------
% Here the document ends, code below is just as sample
%--------------------------------------


  You are writing a test and you are given a two choice question where
  the answer could be either A or B. You assume that the apriori probabilities $P(A) =
  P(B) = 0.5$ (i.e. there is equal a priori chance for each of the answers
  to be correct). After reading the question you estimate
  $P(A|\mbox{question}) = 0.8$. Further you know that the loss function
  has the following form:\newline

 \begin{tabular}{ccccc}
    $W(A, A)$ & = & $W(B, B)$ & = & $-2$,\\
    $W(A, B)$ & = & $W(B, A)$ & = & $4$,\\
    $W(A, ?)$ & = & $W(B, ?)$ & = & $0$,
  \end{tabular}\newline

  where $W(X, Y)$ is the loss function, $X$ is the correct answer, 
  $Y$ your decision, and ``?'' stands for not selecting any of the
  answers (i.e. you say that you do not know).

  \begin{enumerate}
    \item What will be your decision if you wanted to minimalise the
      Bayes risk and why?
    \item At which value of $P(A|\mbox{question})$ it starts to be
      advantageous to start to say ``I do not know''?
  \end{enumerate}

  \hfill {\it (2 points)}

  \vspace*{1em}

  \noindent {\bf Task 2} \newline

  Given $p(x|2) \propto N(0,1)$. Find (approximately using the plot below) the Neyman-Pearson decision strategy (normal state is $1$ and dangerous state is $2$) under the assumption that allowed probability of overlooked danger is 2\% and
  \begin{enumerate}
    \item $p(x|1) \propto N(1,1)$\hfill {\it (1 point)}
    \item $p(x|1) \propto N(1,\sigma)$; $\sigma \gg 1\hfill$ {\it (1 point)}
    \item $p(x|1) \propto N(0,\sigma)$\hfill {\it (1 point)}
  \end{enumerate}
  Explain your calculation. Is there a case when the optimal decision strategy decides for dangerous state on 3 or more disconnected intervals of $X$?


Tracking is formulated as an inference problem. Let $X_i$ be an object's internal state at grame $i$, a random variable, and $Y_i$ the measurements obained in the $i$-th frame. There are three main problems then:
\begin{itemize}
\item {\it Prediction.} Given $y_0, \ldots, y_{i-1}$ compute $P(X_i|Y_0=y_0, \ldots, Y_{i-1} = y_{i-1})$.
\item {\it Data Association.} Identify the measurements from $i$-th frame, which tell us something about $X_i$.
\item {\it Correction.} Given $y_i$ (the relevant ones) compute $P(X_i|Y_0=y_0, \ldots, Y_i = y_i)$.
\end{itemize}
Further, two independence assumptions are made:
\begin{itemize}
\item Only the immediate past matters, i.e. $$P(X_i|X_1, \ldots, X_{i-1}) = P(X_i|X_{i-1})\;.$$
\item Measurements depend only on the current state $$P(Y_i, Y_j, \ldots, Y_k|X_i) = P(Y_i|X_i) P(Y_j, \ldots, Y_k|X_i)\;.$$
\end{itemize}
Without these assumptions, tracking is very difficult. Thanks to these assumptions, tracking has the structure of inference on a hidden Markov model.

{\it Inference.} Done inductively. Lets assume we have $P(X_0)$. Then, correction step is easy
$$P(X_0|Y_0=y_0) = \frac{P(y_0|X_0)P(X_0)}{P(y_0)} \propto P(y_0|X_0)P(X_0)\;.$$
Lets assume we have $P(X_{i-1}|y_0,\ldots,y_{i-1})$. The prediction step is then (using the independence assumptions):
\begin{eqnarray*}
P(X_i|y_0,\ldots,y_{i-1}) &=& \int P(X_i,X_{i-1}|y_0,\ldots,y_{i-1})dX_{i-1}\\
&=& \int P(X_i|X_{i-1},y_0,\ldots,y_{i-1}) P(X_{i-1}|y_0,\ldots,y_{i-1})dX_{i-1}\\
&=& \int P(X_i|X_{i-1})P(X_{i-1}|y_0,\ldots,y_{i-1})dX_{i-1}\;.
\end{eqnarray*}
The correction step is then
\begin{eqnarray*}
P(X_i|y_0,\ldots,y_i) &=& \frac{P(X_i,y_0,\ldots,y_i)}{P(y_0,\ldots,y_i)}\\
&=& \frac{P(y_i|X_i,y_0,\ldots,y_{i-1}) P(X_i|y_0,\ldots,y_{i-1}) P(y_0,\ldots,y_{i-1})}{P(y_0,\ldots,y_i)}\\
&=& P(y_i|X_i) P(X_i|y_0,\ldots,y_{i-1})\frac{P(y_0,\ldots,y_{i-1})}{P(y_0,\ldots,y_i)}\\
&=& \frac{P(y_i|X_i) P(X_i|y_0,\ldots,y_{i-1})}{\int P(y_i|X_i) P(X_i|y_0,\ldots,y_{i-1}) dX_i}
\end{eqnarray*}
With this formulation, the {\bf key issue} is to find a representation of the densities that
\begin{itemize}
\item is sufficiently accurate,
\item allows quick and easy inference.
\end{itemize} 
{\bf Taxonomy} of different cases found in literature:
\begin{itemize}
\item Pdfs are linear, measurement model is linear and noise is Gaussian, then Kalman filter.
\item When the model is non-linear, then extended Kalman filter (not always reliable)
\item Multi-modal representation of $P(x_i|y_0,\ldots,y_i)$ -- Particle filter
\item Determining relevant $y_i$'s -- Data association
\end{itemize}  






